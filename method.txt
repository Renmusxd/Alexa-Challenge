The field of conversational AI is rapidly evolving. Today, proposed architectures limit their scope to only a neural network architecture, and do not focus on integration across broad aspects of the NLP AI research space. Each proposal attempts to utilize one solution for the entire problem at hand. Few proposals have brought multiple architectures together in a way that can provide new, robust response and behavior. Additionally, very little is "hard coded" in these architectures, but hard coded patches can potentially hide some of the imperfections in neural network architectures. Our goal is to bring several state-of-the-art Neural Network (NN) designs together to achieve more coherent, deeper conversations. 

Since we do not know a priori how these architectures will interact with each other, we must develop a methodology to test various iterations of architecture designs. Below we will discuss possible methodologies for determining good architectures that interface with each other. Additionally we will discuss different designs under consideration and how they fit into the architecture provided in our design document.

For each of the modules included in the design document we plan to look into similar alternatives. For example DMNs could be replaced with alternative database queries, or LSTMs could be replaced with TACNTNs (Yu Wu et al. https://arxiv.org/abs/1605.00090). Each of these modules would be trained on equivalent data and incorporated into the overall design. The training of these modules is accomplished using reinforcement learning as proposed in Dan Jurafsky's paper Deep Reinforcement Learning for Dialogue Generation (Dan Jurafsky et al. https://arxiv.org/abs/1606.01541).

Similar architectures and module options can also be interchanged if their response accuracy is similar. As their accuracy may be dependent on input sentence structures keeping both architectures would be worthwhile with a specially trained classifier sending inputs into either option. We would train the classifier to maximize overall accuracy of inputs by selecting one of the options. This input dispatch design could be kept in the final architecture or used solely to analyze weaknesses of each module to further refine training. This methodology can be applied at every step of our design when trying out the various low-level implementations outlined in the next section of this document.

Current examples of these architecture choices in the DD section include topic classifications. Topic classification of text has a rich history from SVMs, LDAs, to even CNNs. Each of these has strengths and weaknesses for analyzing different styles of phrases as well as infering and working around rare word usage. Choosing between classifiers would require experimentation on the specific phrases used in conversation rather than in written text. Should ambiguities arise we can fall back on the topic database's reported confidence to determine which of the topics most accurately represents and responds to the query.

The topics themselves have multiple options. Though DMNs offer many benefits we should allow the model to operate independent of the choice of information retrieval technique used. This should apply at the level of the CC to DD connection as well as within the DD module itself. Topic classification should dispatch the query to the relevant topic(s), we can then rate which choice of topic is most suited by comparing the generated output to articles in the field. For example a question about the age of a movie star can be compared to phrases in that star's wikipedia page to gauge accuracy. As such the query "I keep hearing about [TOPIC], what is that?" should return an answer similar to some information available in the wikipedia article for that topic. Training should reflect this by comparing results from the socialbot to phrases available in the article (wikipedia here is an example of one potential database, as the scale of the project grows we would, of course, build a custom database from freely available information to optimize this information retrieval, like an english language version of Freebase).

We extend the DCGM architecture by incorporating additional persona and user sentiment embeddings into our model. We propose representing each individual speaker as a vector, which encodes background information and speaking style. These vectors can be learned jointly with word embeddings by back propagating word predication errors during training. It is also possible to capture interaction patterns within conversation by linearly combining two speakers’ vectors. A persona vector space would serve to cluster users by their traits without explicit annotations. 

In the same way we can inject a sentiment vector into our hidden layer, which we will generate by learning sentence sentiment embeddings within a vector space (http://www.aclweb.org/anthology/P08-2034) or by training a recursive auto-encoder on Stanford’s Experience Project Dataset.

Ultimately, we will better predict personalized responses by enriching baseline DCGM models by training persona and sentiment vectors and injecting them directly into the hidden layer at each time step.

The choice of word embedding as input to each module also affords as many options. Our current model is using Fasttext plus multiple layers of LSTMs to map synonyms together in our embedding (https://arxiv.org/abs/1607.01759v2 and https://arxiv.org/abs/1607.04606). The aim of this decision is to understand synonyms in the user's dialogue for referencing knowledge we have stored: terms such as "old" and "age" will be mapped close together to simplify the operation of the topic modules and facilitate information retrieval. This method also implements a softmax which makes in conducive to pointer sentinel mixture model (https://arxiv.org/abs/1609.07843) which has some promising benefits. An alternate approach is to use a TACNTNs (https://arxiv.org/abs/1605.00090) to make use of the convolutional neural networks analysis of word ordering. While this may make rare words more difficult to handle it would perhaps allow for a deeper understanding of the user's sentence based off the ordering as well as word choice.

Our model assumes access to the most likely phrase from the Alexa Skill Kit. We intend to host our Socialbot on AWS Lambda in a general callback function that handles all responses given a “heard” text query. In the callback, our model will take in this user query and generate a text response for Alexa.