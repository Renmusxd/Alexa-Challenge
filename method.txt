The state of affairs in applying neural networks (NN’s) to natural language processing (NLP), particularly in the realm of conversational nets and social or chat bots is rapidly evolving. Today, proposed architectures limit their scope to only a neural network architecture, and do not focus on integration across broad aspects of the NLP AI research space. Each proposal attempts to utilize one solution for the entire problem at hand. Few proposals have brought multiple architectures together in a way that can provide new, robust response and behavior. Additionally, very little is "hard coded" in these architectures, but hard coded patches can potentially hide some of the imperfections in neural network architectures. Our goal is to bring several state-of-the-art NN designs together to achieve more coherent, deeper conversations. 

Since we do not know a priori how these architectures will interact with each other, we must develop a methodology to test various iterations of architecture designs. Below we will discuss possible methodologies for determining good architectures that interface with each other. Additionally we will discuss different designs under consideration and how they fit into the architecture provided in our design document.

For each of the modules included in the design document we plan to look into similar alternatives. For example DMNs could be replaced with alternative database queries, or LSTMs could be replaced with TACNTNs. Each of these modules would be trained on equivalent data and incorporated into the overall design. The training of these modules is accomplished using reinforcement learning as proposed in Dan Jurafsky's paper Deep Reinforcement Learning for Dialogue Generation (Dan Jurafsky et al. https://arxiv.org/abs/1606.01541).

Similar architectures and module choices can also be chosen between should their response accuracy be within a percentage point, but their accuracy may differ on different phrases in the input space. Hence, keeping both architectures would be worthwhile with some classifier sending inputs into either A or B. However, this presupposes the necessity and possibility of a classifier that can accurately distinguish between these two types of inputs. Hence, multiple classification techniques would need to be adopted to determine the feasibility of classification and of using multiple architectures simultaneously. Furthermore, we must run computational analyses to determine that we can run the architectures in parallel without too much overhead or decreased latency of our program.

This methodology can be applied at every step of our design when trying out the various low-level implementations outlined in the next section of this document.

It is needless to say that this methodology can be app potentially applied to hyperparameter tuning. However, since this doesn’t happen for a given architecture in a research paper, we find it unreasonable to assume that it would work in this context and so forgo it for our progress in implementing this project
